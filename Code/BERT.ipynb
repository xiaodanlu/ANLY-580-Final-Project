{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data\n",
    "data  = pd.read_csv(\"../Data/Combined.csv\")\n",
    "\n",
    "# Split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "X_train = train['Tweets_clean_more']\n",
    "y_train = train['Label']\n",
    "X_test = test['Tweets_clean_more']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization & Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer_1 = Tokenizer(num_words=5000, oov_token=\"\")\n",
    "tokenizer_1.fit_on_texts(X_train)\n",
    "X_train_tokenized = tokenizer_1.texts_to_sequences(X_train)\n",
    "X_test_tokenized = tokenizer_1.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad the data\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train_tokenized, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test_tokenized, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Matrix - GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvocab_size = len(tokenizer.word_index) + 1\\n\\nembedding_index = dict()\\nf = open(\"../Input Source/golve.twitter.27B/glove.twitter.27B.200d.txt\")\\nfor line in f:\\n    values = line.split()\\n    word = values[0]\\n    coefs = np.asarray(values[1:], dtype=\\'float32\\')\\n    embedding_index[word] = coefs\\nf.close()\\nprint(\\'Loaded %s word vectors.\\' % len(embedding_index))\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "embedding_index = dict()\n",
    "f = open(\"../Input Source/golve.twitter.27B/glove.twitter.27B.200d.txt\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embedding_index))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 160\n",
    "lr = 1e-5 #\n",
    "num_epochs = 3 \n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in /Users/sherry/opt/anaconda3/envs/anly-580/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/sherry/opt/anaconda3/envs/anly-580/lib/python3.10/site-packages (from tensorflow_hub) (1.23.3)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Users/sherry/opt/anaconda3/envs/anly-580/lib/python3.10/site-packages (from tensorflow_hub) (3.19.4)\n"
     ]
    }
   ],
   "source": [
    "#-wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
    "\n",
    "!pip install tensorflow_hub\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512, lr=1e-5):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /Users/sherry/opt/anaconda3/envs/anly-580/lib/python3.10/site-packages (1.0.4)\n",
      "Requirement already satisfied: six in /Users/sherry/opt/anaconda3/envs/anly-580/lib/python3.10/site-packages (from bert-tensorflow) (1.16.0)\n"
     ]
    },
    {
     "ename": "UnparsedFlagAccessError",
     "evalue": "Trying to access flag --preserve_unused_tokens before flags were parsed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnparsedFlagAccessError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/sherry/ANLY-580-Final-Project-1/Code/BERT.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sherry/ANLY-580-Final-Project-1/Code/BERT.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m X_test_Bert \u001b[39m=\u001b[39m test[\u001b[39m'\u001b[39m\u001b[39mTweets_clean_more\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sherry/ANLY-580-Final-Project-1/Code/BERT.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m y_test \u001b[39m=\u001b[39m test[\u001b[39m'\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sherry/ANLY-580-Final-Project-1/Code/BERT.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_input \u001b[39m=\u001b[39m bert_encode(X_train_Bert, tokenizer, max_len\u001b[39m=\u001b[39;49mmaxlen)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sherry/ANLY-580-Final-Project-1/Code/BERT.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m test_input \u001b[39m=\u001b[39m bert_encode(X_test_Bert, tokenizer, max_len\u001b[39m=\u001b[39mmaxlen)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sherry/ANLY-580-Final-Project-1/Code/BERT.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m train_labels \u001b[39m=\u001b[39m y_train\n",
      "\u001b[1;32m/Users/sherry/ANLY-580-Final-Project-1/Code/BERT.ipynb Cell 12\u001b[0m in \u001b[0;36mbert_encode\u001b[0;34m(texts, tokenizer, max_len)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sherry/ANLY-580-Final-Project-1/Code/BERT.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m all_segments \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sherry/ANLY-580-Final-Project-1/Code/BERT.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sherry/ANLY-580-Final-Project-1/Code/BERT.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sherry/ANLY-580-Final-Project-1/Code/BERT.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     text \u001b[39m=\u001b[39m text[:max_len\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sherry/ANLY-580-Final-Project-1/Code/BERT.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     input_sequence \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m[CLS]\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m text \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m[SEP]\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly-580/lib/python3.10/site-packages/bert/tokenization.py:192\u001b[0m, in \u001b[0;36mFullTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[1;32m    191\u001b[0m   split_tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 192\u001b[0m   \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbasic_tokenizer\u001b[39m.\u001b[39;49mtokenize(text):\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m preserve_token(token, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab):\n\u001b[1;32m    194\u001b[0m       split_tokens\u001b[39m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly-580/lib/python3.10/site-packages/bert/tokenization.py:237\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    235\u001b[0m split_tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m    236\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m orig_tokens:\n\u001b[0;32m--> 237\u001b[0m   \u001b[39mif\u001b[39;00m preserve_token(token, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab):\n\u001b[1;32m    238\u001b[0m     split_tokens\u001b[39m.\u001b[39mappend(token)\n\u001b[1;32m    239\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly-580/lib/python3.10/site-packages/bert/tokenization.py:41\u001b[0m, in \u001b[0;36mpreserve_token\u001b[0;34m(token, vocab)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreserve_token\u001b[39m(token, vocab):\n\u001b[1;32m     40\u001b[0m   \u001b[39m\"\"\"Returns True if the token should forgo tokenization and be preserved.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m FLAGS\u001b[39m.\u001b[39;49mpreserve_unused_tokens:\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     43\u001b[0m   \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m vocab:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly-580/lib/python3.10/site-packages/absl/flags/_flagvalues.py:478\u001b[0m, in \u001b[0;36mFlagValues.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    476\u001b[0m   \u001b[39mreturn\u001b[39;00m fl[name]\u001b[39m.\u001b[39mvalue\n\u001b[1;32m    477\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m   \u001b[39mraise\u001b[39;00m _exceptions\u001b[39m.\u001b[39mUnparsedFlagAccessError(\n\u001b[1;32m    479\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mTrying to access flag --\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m before flags were parsed.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m name)\n",
      "\u001b[0;31mUnparsedFlagAccessError\u001b[0m: Trying to access flag --preserve_unused_tokens before flags were parsed."
     ]
    }
   ],
   "source": [
    "! pip install bert-tensorflow\n",
    "from bert import tokenization\n",
    "\n",
    "#%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "X_train_Bert = train['Tweets_clean_more']\n",
    "y_train = train['Label']\n",
    "X_test_Bert = test['Tweets_clean_more']\n",
    "y_test = test['Label']\n",
    "\n",
    "train_input = bert_encode(X_train_Bert, tokenizer, max_len=maxlen)\n",
    "test_input = bert_encode(X_test_Bert, tokenizer, max_len=maxlen)\n",
    "train_labels = y_train\n",
    "test_labels = y_test\n",
    "\n",
    "bert_model = build_model(bert_layer, max_len=maxlen, lr=lr)\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('bertmodel.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "bert_history = bert_model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_data=(val_input, val_labels),\n",
    "    epochs=num_epochs,\n",
    "    callbacks=[checkpoint], \n",
    "    #class_weight=class_weight,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('anly-580')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "632cdf940b59264b613ce36e6ff44b18344167f8957438a92a721914f37336e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
